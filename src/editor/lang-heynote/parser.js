// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {noteContent} from "./external-tokens.js"
export const parser = LRParser.deserialize({
  version: 14,
  states: "!jQQOPOOOVOPO'#C`O[OQO'#C_OOOO'#Cc'#CcQQOPOOOaOPO,58zOOOO,58y,58yOOOO-E6a-E6aOfOPO1G.fOOOQ7+$Q7+$QOnOPO7+$QOOOQ<<Gl<<Gl",
  stateData: "s~OXPO~OYTO~OPUO~OTWO~OUYOXXO~OXZO~O",
  goto: "gWPPPX]PPaTROSTQOSQSORVS",
  nodeNames: "âš  NoteContent Document Note NoteDelimiter NoteLanguage Auto",
  maxTerm: 10,
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData: "'S~RZYZt}!Oy#V#W!U#[#]!g#^#_!y#`#a$d#a#b$|#d#e%r#g#h&_#h#i&e%&x%&y&q~yOX~~|P#T#U!P~!UOU~~!XP#g#h![~!_P#g#h!b~!gOT~~!jP#h#i!m~!pP#a#b!s~!vP#`#a!b~!|Q#T#U#S#g#h$W~#VP#j#k#Y~#]P#T#U#`~#ePT~#g#h#h~#kP#V#W#n~#qP#f#g#t~#wP#]#^#z~#}P#d#e$Q~$TP#h#i!b~$ZP#c#d$^~$aP#b#c!b~$gP#X#Y$j~$mP#n#o$p~$sP#X#Y$v~$yP#f#g!b~%PP#T#U%S~%VP#f#g%Y~%]P#_#`%`~%cP#W#X%f~%iP#c#d%l~%oP#k#l$^~%uQ#[#]%{#m#n&R~&OP#d#e!b~&UP#h#i&X~&[P#[#]$W~&bP#e#f!s~&hP#X#Y&k~&nP#l#m$Q~&tP%&x%&y&w~&zP%&x%&y&}~'SOY~",
  tokenizers: [0, noteContent],
  topRules: {"Document":[0,2]},
  tokenPrec: 0
})
